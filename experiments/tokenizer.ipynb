{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from typing import Union, List\n",
    "\n",
    "class CharacterLevelTokenizer:\n",
    "    def __init__(self, max_len: int) -> None:\n",
    "        self.max_len = max_len\n",
    "        self.vocab = {\n",
    "            '<PAD>': 0,\n",
    "            '<MASK>': 1,\n",
    "        }\n",
    "        offset = len(self.vocab)\n",
    "        for k, v in zip('abcdefghijklmnopqrstuvwxyz', range(offset, 27+offset)):\n",
    "            self.vocab[k] = v\n",
    "        \n",
    "    def tokenize(self, word: str, labels: List[str]=None, return_pt=True) -> Union[torch.Tensor, List[int]]:\n",
    "        word = map(lambda x: x if x != '_' else '<MASK>', list(word))\n",
    "\n",
    "        res = []\n",
    "        for c in word:\n",
    "            res.append(self.vocab[c])\n",
    "        \n",
    "        res += [self.vocab['<PAD>']] * (self.max_len - len(res))\n",
    "        \n",
    "        if labels:\n",
    "            labels = list(map(lambda x: self.vocab.get(x) if x else -100, labels))\n",
    "            labels = labels + [-100] * (self.max_len - len(labels))\n",
    "            labels = torch.tensor(labels).long()\n",
    "            res = torch.tensor(res).long()\n",
    "            return res, labels\n",
    "        \n",
    "        if return_pt:\n",
    "            return torch.tensor(res).long()\n",
    "        \n",
    "        return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = CharacterLevelTokenizer(max_len=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "from typing import Tuple, List\n",
    "\n",
    "def random_character_masker(word: str) -> Tuple[str, List[str]]:\n",
    "    unique_chars = set(word)\n",
    "    num_to_mask = random.randint(1, len(unique_chars))\n",
    "    chars_to_mask = random.sample(unique_chars, num_to_mask)\n",
    "    labels = [None for _ in range(len(word))]\n",
    "    for c in chars_to_mask:\n",
    "        for i, cc in enumerate(word):\n",
    "            if c == cc:\n",
    "                labels[i] = c\n",
    "        word = word.replace(c, '_')\n",
    "    return word, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ('shi_am', [None, None, None, 'v', None, None])\n",
      "1 ('_h___m', ['s', None, 'i', 'v', 'a', None])\n",
      "2 ('shi_a_', [None, None, None, 'v', None, 'm'])\n",
      "3 ('____a_', ['s', 'h', 'i', 'v', None, 'm'])\n",
      "4 ('___v__', ['s', 'h', 'i', None, 'a', 'm'])\n",
      "5 ('______', ['s', 'h', 'i', 'v', 'a', 'm'])\n",
      "6 ('___v_m', ['s', 'h', 'i', None, 'a', None])\n",
      "7 ('s__v__', [None, 'h', 'i', None, 'a', 'm'])\n",
      "8 ('s__v_m', [None, 'h', 'i', None, 'a', None])\n",
      "9 ('______', ['s', 'h', 'i', 'v', 'a', 'm'])\n",
      "10 ('shi_a_', [None, None, None, 'v', None, 'm'])\n",
      "11 ('sh____', [None, None, 'i', 'v', 'a', 'm'])\n",
      "12 ('____am', ['s', 'h', 'i', 'v', None, None])\n",
      "13 ('s_____', [None, 'h', 'i', 'v', 'a', 'm'])\n",
      "14 ('__i_a_', ['s', 'h', None, 'v', None, 'm'])\n",
      "15 ('_h____', ['s', None, 'i', 'v', 'a', 'm'])\n",
      "16 ('shi___', [None, None, None, 'v', 'a', 'm'])\n",
      "17 ('______', ['s', 'h', 'i', 'v', 'a', 'm'])\n",
      "18 ('shi_a_', [None, None, None, 'v', None, 'm'])\n",
      "19 ('_h____', ['s', None, 'i', 'v', 'a', 'm'])\n"
     ]
    }
   ],
   "source": [
    "word='shivam'\n",
    "for i in range(20):\n",
    "    print(i, random_character_masker(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 (tensor([20,  1, 10, 23,  2,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]), tensor([-100,    9, -100, -100, -100,   14, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100]))\n",
      "1 (tensor([20,  9, 10,  1,  2, 14,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]), tensor([-100, -100, -100,   23, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100]))\n",
      "2 (tensor([1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([  20,    9,   10,   23,    2,   14, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100]))\n",
      "3 (tensor([ 1,  1, 10,  1,  1, 14,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]), tensor([  20,    9, -100,   23,    2, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100]))\n",
      "4 (tensor([ 1,  9, 10,  1,  1, 14,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]), tensor([  20, -100, -100,   23,    2, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100]))\n",
      "5 (tensor([ 1,  1,  1, 23,  1,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]), tensor([  20,    9,   10, -100,    2,   14, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100]))\n",
      "6 (tensor([20,  9, 10,  1,  1,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]), tensor([-100, -100, -100,   23,    2,   14, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100]))\n",
      "7 (tensor([ 1,  1,  1,  1,  1, 14,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]), tensor([  20,    9,   10,   23,    2, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100]))\n",
      "8 (tensor([1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([  20,    9,   10,   23,    2,   14, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100]))\n",
      "9 (tensor([20,  9, 10,  1,  2, 14,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]), tensor([-100, -100, -100,   23, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100]))\n",
      "10 (tensor([1, 9, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([  20, -100,   10,   23,    2,   14, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100]))\n",
      "11 (tensor([1, 9, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([  20, -100,   10,   23,    2,   14, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100]))\n",
      "12 (tensor([20,  1,  1, 23,  1, 14,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]), tensor([-100,    9,   10, -100,    2, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100]))\n",
      "13 (tensor([ 1,  1,  1, 23,  1,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]), tensor([  20,    9,   10, -100,    2,   14, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100]))\n",
      "14 (tensor([20,  1,  1, 23,  2, 14,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]), tensor([-100,    9,   10, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100]))\n",
      "15 (tensor([20,  9, 10, 23,  2,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]), tensor([-100, -100, -100, -100, -100,   14, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100]))\n",
      "16 (tensor([20,  1,  1, 23,  2,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]), tensor([-100,    9,   10, -100, -100,   14, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100]))\n",
      "17 (tensor([20,  1,  1,  1,  2,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]), tensor([-100,    9,   10,   23, -100,   14, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100]))\n",
      "18 (tensor([1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([  20,    9,   10,   23,    2,   14, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100]))\n",
      "19 (tensor([ 1,  9, 10, 23,  2, 14,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]), tensor([  20, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100]))\n"
     ]
    }
   ],
   "source": [
    "word='shivam'\n",
    "for i in range(20):\n",
    "    masked_word, label = random_character_masker(word)\n",
    "    print(i, tokenizer.tokenize(masked_word, label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, word_file: str, split: str, max_len: int):\n",
    "        with open(word_file, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "\n",
    "        lines = list(map(lambda x: x.replace('\\n', ''), lines))\n",
    "        self.split = split\n",
    "        train, test = train_test_split(lines, test_size=0.2, random_state=42)\n",
    "        del lines\n",
    "\n",
    "        if split == 'train':\n",
    "            self.words = train\n",
    "        else:\n",
    "            val, test = train_test_split(test, test_size=0.5, random_state=42)\n",
    "            if split == 'val':\n",
    "                self.words = val\n",
    "            elif split == 'test':\n",
    "                self.words = test\n",
    "            else:\n",
    "                raise ValueError(f'Split should be in train/val/test but got {split}.')\n",
    "        \n",
    "        self.tokenizer = CharacterLevelTokenizer(max_len=max_len)\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.words)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        word = self.words[index]\n",
    "        masked_word, label = random_character_masker(word)\n",
    "        masked_word, label = self.tokenizer.tokenize(masked_word, label)\n",
    "        attention_mask = torch.ones_like(masked_word)\n",
    "        idx = masked_word == self.tokenizer.vocab['<PAD>']\n",
    "        attention_mask[idx] = 0\n",
    "        return {\n",
    "            'input_ids': masked_word,\n",
    "            'labels': label,\n",
    "            'attention_mask': attention_mask\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([ 6, 25,  9,  1,  1,  1,  1,  1, 21,  1,  1,  8,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
       " 'labels': tensor([-100, -100, -100,   10,   13,    2,   19,    2, -100,   10,   15, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = CustomDataset(\n",
    "    word_file='words_250000_train.txt',\n",
    "    split='train',\n",
    "    max_len=64\n",
    ")\n",
    "next(iter(ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([ 1,  1,  1,  1, 13,  2, 19,  2,  1,  1,  1,  1,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0]), 'labels': tensor([   6,   25,    9,   10, -100, -100, -100, -100,   21,   10,   15,    8,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}\n",
      "{'input_ids': tensor([ 1, 25,  9, 10,  1,  1,  1,  1, 21, 10, 15,  1,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0]), 'labels': tensor([   6, -100, -100, -100,   13,    2,   19,    2, -100, -100, -100,    8,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}\n",
      "{'input_ids': tensor([ 6,  1,  1, 10,  1,  2, 19,  2,  1, 10, 15,  1,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0]), 'labels': tensor([-100,   25,    9, -100,   13, -100, -100, -100,   21, -100, -100,    8,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print(next(iter(ds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "181840\n",
      "22730\n",
      "22730\n"
     ]
    }
   ],
   "source": [
    "for split in ['train', 'val', 'test']:\n",
    "    ds = CustomDataset(\n",
    "        word_file='words_250000_train.txt',\n",
    "        split=split,\n",
    "        max_len=64\n",
    "    )\n",
    "    print(len(ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from transformers import BertModel, BertConfig\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name       | Type      | Params\n",
      "-----------------------------------------\n",
      "0 | bert       | BertModel | 211 K \n",
      "1 | classifier | Linear    | 1.9 K \n",
      "-----------------------------------------\n",
      "213 K     Trainable params\n",
      "0         Non-trainable params\n",
      "213 K     Total params\n",
      "0.852     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 5683/5683 [10:08<00:00,  9.34it/s, v_num=8]      "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 5683/5683 [10:08<00:00,  9.34it/s, v_num=8]\n"
     ]
    }
   ],
   "source": [
    "import pdb\n",
    "\n",
    "class BertClassifier(pl.LightningModule):\n",
    "    def __init__(self, config, num_classes):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel(config)\n",
    "        self.classifier = torch.nn.Linear(config.hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs['last_hidden_state']\n",
    "        # print(pooled_output.shape)\n",
    "        logits = self.classifier(pooled_output)\n",
    "        return logits\n",
    "    \n",
    "    def loss_fn(self, logits, labels):\n",
    "        idx = labels != -100\n",
    "        # pdb.set_trace()\n",
    "        return torch.nn.functional.cross_entropy(logits[idx], labels[idx])\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=0.02)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        logits = self(batch['input_ids'], batch['attention_mask'])\n",
    "        loss = self.loss_fn(logits, batch['labels'])\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        logits = self(batch['input_ids'], batch['attention_mask'])\n",
    "        loss = self.loss_fn(logits, batch['labels'])\n",
    "        self.log('val_loss', loss)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        logits = self(batch['input_ids'], batch['attention_mask'])\n",
    "        loss = torch.nn.functional.cross_entropy(outputs, batch['labels'])\n",
    "        self.log('test_loss', loss)\n",
    "\n",
    "num_classes = 29  # Replace with the actual number of classes\n",
    "max_len = 64\n",
    "# Load the BERT configuration\n",
    "config = BertConfig(\n",
    "    vocab_size=num_classes,  # Replace with the actual vocab size\n",
    "    hidden_size=64,\n",
    "    num_hidden_layers=6,\n",
    "    num_attention_heads=4,\n",
    "    intermediate_size=64*2,\n",
    "    max_position_embeddings=max_len,\n",
    ")\n",
    "\n",
    "# Instantiate the BERT-based classifier\n",
    "bert_classifier = BertClassifier(config, num_classes)\n",
    "\n",
    "# Create train, validation, and test datasets\n",
    "train_dataset = CustomDataset(\n",
    "    'words_250000_train.txt', \n",
    "    split='train', \n",
    "    max_len=max_len\n",
    ")\n",
    "val_dataset = CustomDataset(\n",
    "    'words_250000_train.txt', \n",
    "    split='val', \n",
    "    max_len=max_len\n",
    ")\n",
    "test_dataset = CustomDataset(\n",
    "    'words_250000_train.txt', \n",
    "    split='test', \n",
    "    max_len=max_len\n",
    ")\n",
    "\n",
    "# Create dataloaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=64)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64)\n",
    "\n",
    "# Create a PyTorch Lightning Trainer\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=1,\n",
    "    devices=1, \n",
    "    accelerator=\"cpu\",\n",
    ")\n",
    "\n",
    "# Train the model using the Trainer\n",
    "trainer.fit(bert_classifier, train_dataloader, val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "logits = bert_classifier(\n",
    "    input_ids=batch['input_ids'], \n",
    "    attention_mask=batch['attention_mask']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 64, 27])"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 64, 1])"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "batch['label'].unsqueeze(-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([174, 27])\n",
      "torch.Size([174])\n"
     ]
    }
   ],
   "source": [
    "idx = batch['label'] != -100\n",
    "loss = torch.nn.functional.cross_entropy(logits[idx], batch['label'][idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pt2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
